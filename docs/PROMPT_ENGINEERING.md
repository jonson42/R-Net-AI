# Prompt Engineering Strategy & Best Practices

## üìä Analysis of Current Implementation

### Strengths ‚úÖ
1. **Structured JSON Response Format** - Enforces consistent output parsing
2. **Tech Stack Awareness** - Dynamically adjusts based on chosen technologies
3. **Retry Logic with Exponential Backoff** - Handles rate limits gracefully
4. **Image Preprocessing** - Validates, resizes, and optimizes images
5. **Fallback Mechanism** - Graceful degradation when parsing fails
6. **Comprehensive Error Handling** - Catches specific OpenAI exceptions

### Areas for Improvement üîß
1. **Prompt Depth** - Original prompts were brief; enhanced version adds extensive guidance
2. **Security Emphasis** - Enhanced template explicitly requires security measures
3. **Code Completeness** - Stronger "no placeholders" enforcement
4. **Architecture Guidance** - Clearer separation of concerns instructions
5. **Framework-Specific Details** - Tailored requirements per tech choice
6. **Performance Optimization** - Explicit performance best practices
7. **Testing Requirements** - Mandatory test inclusion

---

## üéØ Enhanced Prompt Strategy

### 1. **System Prompt Architecture**

The enhanced system prompt uses a multi-layered approach:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ROLE DEFINITION (Expert Persona)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   PROJECT CONTEXT (Stack, Name, Type)  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   RESPONSE FORMAT (Strict JSON Schema) ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   CODE REQUIREMENTS (Comprehensive)     ‚îÇ
‚îÇ   ‚Ä¢ Completeness                        ‚îÇ
‚îÇ   ‚Ä¢ Security                            ‚îÇ
‚îÇ   ‚Ä¢ Architecture                        ‚îÇ
‚îÇ   ‚Ä¢ Frontend Best Practices             ‚îÇ
‚îÇ   ‚Ä¢ Backend Best Practices              ‚îÇ
‚îÇ   ‚Ä¢ Database Design                     ‚îÇ
‚îÇ   ‚Ä¢ Testing                             ‚îÇ
‚îÇ   ‚Ä¢ Performance                         ‚îÇ
‚îÇ   ‚Ä¢ Documentation                       ‚îÇ
‚îÇ   ‚Ä¢ Deployment                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   FRAMEWORK-SPECIFIC REQUIREMENTS       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   EDGE CASES & QUALITY CHECKLIST        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 2. **Key Improvements in Enhanced Template**

#### A. **Explicit Expert Persona**
```
"You are a world-class senior full-stack architect with 15+ years experience..."
```
**Why:** Research shows LLMs perform better when assigned specific expert roles with credentials.

#### B. **Visual Formatting with Separators**
```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
SECTION TITLE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```
**Why:** Clear visual boundaries help the model parse different instruction sections.

#### C. **Checkboxes and Lists**
```
‚úì Item 1
‚úì Item 2
‚úì Item 3
```
**Why:** Creates a mental "checklist" that improves compliance with requirements.

#### D. **Negative Instructions (What NOT to Do)**
```
‚ùå No placeholders like "// Add logic here"
‚ùå No TODOs or incomplete implementations
‚ùå Never hardcode secrets
```
**Why:** Explicit negative examples reduce unwanted patterns.

#### E. **Quantifiable Targets**
```
‚Ä¢ Test coverage: >70% on backend, >60% on frontend
‚Ä¢ Max function length: 50 lines
‚Ä¢ Bundle size: <200KB initial
```
**Why:** Specific numbers create measurable quality criteria.

#### F. **Framework-Specific Guidance**
```python
def _get_frontend_specific_requirements(frontend: str) -> str:
    # Returns tailored instructions for React, Vue, Angular, Svelte
```
**Why:** Generic advice produces generic code; specific guidance produces idiomatic code.

---

## üî¨ Prompt Engineering Principles Applied

### 1. **Chain of Thought (CoT) Prompting**
The user prompt includes step-by-step image analysis instructions:
```
1. Layout Structure ‚Üí Identify components
2. Interactive Elements ‚Üí List all inputs/buttons
3. Data Entities ‚Üí Infer database schema
4. User Flows ‚Üí Map navigation paths
5. Visual Design ‚Üí Extract theme
```
**Effect:** Encourages the model to reason through the problem systematically.

### 2. **Few-Shot Learning (Implicit)**
The JSON format example shows:
```json
{
  "files": [
    {
      "path": "frontend/src/App.tsx",
      "content": "// COMPLETE file content - NO placeholders",
      "description": "Main application component..."
    }
  ]
}
```
**Effect:** Model learns the desired output structure through example.

### 3. **Constrained Generation**
Explicit format requirements:
```
"Return ONLY a valid JSON object. No markdown, no explanations outside the JSON."
```
**Effect:** Reduces likelihood of unparseable responses.

### 4. **Task Decomposition**
Breaking down requirements into:
- Completeness requirements
- Security requirements  
- Architecture requirements
- Frontend requirements
- Backend requirements
- Database requirements
- Testing requirements
- Performance requirements
- Documentation requirements
- Deployment requirements

**Effect:** Comprehensive coverage without overwhelming a single instruction block.

### 5. **Repetition of Critical Instructions**
Security requirements appear in:
1. General requirements section
2. Backend-specific section
3. Edge cases section
4. Final checklist

**Effect:** Reinforcement increases compliance with critical requirements.

---

## üé® Usage Examples

### Example 1: Student Management System

```python
from services.prompt_templates import EXAMPLE_PROMPTS, PromptTemplateEngine

# Use pre-built example
description = EXAMPLE_PROMPTS["student_management"]

# Or build custom
system_prompt = PromptTemplateEngine.create_enhanced_system_prompt(
    tech_stack=TechStack(frontend="React", backend="FastAPI", database="PostgreSQL"),
    project_name="student-management-system",
    app_type="crud"
)

user_prompt = PromptTemplateEngine.create_enhanced_user_prompt(
    description=description,
    tech_stack=tech_stack
)
```

### Example 2: Specialized CRUD App

```python
# For a Product Inventory System
entity_fields = {
    "name": "string",
    "sku": "string (unique)",
    "description": "text",
    "price": "decimal(10,2)",
    "stock_quantity": "integer",
    "category_id": "foreign key",
    "is_active": "boolean",
    "created_at": "timestamp",
    "updated_at": "timestamp"
}

crud_prompt = PromptTemplateEngine.create_specialized_prompt_for_crud_app(
    entity_name="Product",
    fields=entity_fields,
    tech_stack=tech_stack,
    project_name="inventory-system"
)
```

---

## üìà Expected Output Quality Improvements

| Metric | Before | After Enhanced Template |
|--------|--------|------------------------|
| **Files Generated** | 5-8 files | 15-25 files |
| **Code Completeness** | 60-70% | 90-95% |
| **Security Implementation** | Basic/Missing | Comprehensive |
| **Test Coverage** | Rarely included | 3-5 test files |
| **Documentation** | Basic README | README + API + Architecture docs |
| **Production Readiness** | ‚ùå | ‚úÖ Docker, env configs, migrations |
| **Framework Best Practices** | Generic | Idiomatic to stack |

---

## üîß Customization Guide

### Adding New Application Types

Edit `prompt_templates.py`:

```python
@staticmethod
def create_enhanced_system_prompt(
    tech_stack: TechStack, 
    project_name: str,
    app_type: str = "general"  # ‚Üê Add your type here
) -> str:
    # Add custom requirements for your app_type
    if app_type == "dashboard":
        # Add dashboard-specific requirements
        pass
    elif app_type == "api_only":
        # Add API-only requirements
        pass
```

### Adding New Frameworks

```python
@staticmethod
def _get_frontend_specific_requirements(frontend: str) -> str:
    requirements = {
        "React": "...",
        "Vue": "...",
        "YourNewFramework": """
        ‚Ä¢ Specific requirement 1
        ‚Ä¢ Specific requirement 2
        """
    }
```

### Adjusting Quality Thresholds

```python
‚Ä¢ Test coverage: >{YOUR_THRESHOLD}% on backend
‚Ä¢ Max function length: {YOUR_MAX} lines
‚Ä¢ Bundle size: <{YOUR_LIMIT}KB initial
```

---

## üß™ Testing Your Prompts

### Method 1: Unit Test Specific Sections

```python
def test_system_prompt_includes_security():
    prompt = PromptTemplateEngine.create_enhanced_system_prompt(...)
    assert "JWT" in prompt
    assert "bcrypt" in prompt
    assert "SQL injection" in prompt
```

### Method 2: End-to-End Validation

```python
async def test_generation_quality():
    result = await openai_service.generate_code(...)
    
    # Check completeness
    assert len(result['files']) >= 15
    
    # Check for placeholders
    for file in result['files']:
        assert "TODO" not in file['content']
        assert "// Add" not in file['content']
    
    # Check required files
    file_paths = [f['path'] for f in result['files']]
    assert any('README.md' in p for p in file_paths)
    assert any('Dockerfile' in p for p in file_paths)
```

### Method 3: Manual Quality Assessment

Use this checklist after generation:

- [ ] All files have complete, functional code
- [ ] Security measures implemented (auth, validation, hashing)
- [ ] Database schema includes indexes and constraints
- [ ] Tests included for critical paths
- [ ] Documentation is comprehensive
- [ ] Docker/deployment configs present
- [ ] No hardcoded secrets or credentials
- [ ] Error handling throughout
- [ ] Responsive design implemented
- [ ] Code follows framework conventions

---

## üìö References & Further Reading

1. **OpenAI Best Practices**: https://platform.openai.com/docs/guides/prompt-engineering
2. **Chain of Thought Prompting**: Wei et al., 2022
3. **Few-Shot Learning**: Brown et al., 2020
4. **Constitutional AI**: Bai et al., 2022
5. **Prompt Patterns Catalog**: White et al., 2023

---

## üöÄ Quick Start

### 1. Use Enhanced Template (Recommended)

The `openai_service.py` is already updated to use `PromptTemplateEngine` by default.

### 2. Test with Example Prompts

```bash
# From your backend directory
python3
>>> from services.prompt_templates import EXAMPLE_PROMPTS
>>> print(EXAMPLE_PROMPTS["student_management"])
```

### 3. Call API with Rich Description

Use the example prompts as templates for your API calls:

```typescript
// In your VS Code extension
const description = `
Create a comprehensive Student Management System with:
- Student registration and profiles
- Course enrollment and scheduling
- Grade management and transcripts
- Attendance tracking with reports
... (see EXAMPLE_PROMPTS for full template)
`;

const response = await apiService.generateCode({
  image_data: base64Image,
  description: description,
  tech_stack: { frontend: "React", backend: "FastAPI", database: "PostgreSQL" },
  project_name: "student-mgmt-system"
});
```

---

## üí° Pro Tips

1. **Be Specific in Descriptions**: More detail = better output
2. **Use Visual Mockups**: Images dramatically improve UI accuracy
3. **Iterate**: Generate ‚Üí Review ‚Üí Refine description ‚Üí Regenerate
4. **Combine Templates**: Mix example prompts for hybrid features
5. **Provide Context**: Mention target users, scale, special requirements
6. **Version Control Prompts**: Track what descriptions produce best results

---

## ü§ù Contributing

To improve the prompt templates:

1. Test with diverse application types
2. Document what works (and what doesn't)
3. Submit examples of high-quality generations
4. Suggest new framework-specific requirements
5. Share edge cases that need better handling

---

**Last Updated**: November 8, 2025
**Version**: 2.0 (Enhanced Template)
**Maintained By**: R-Net AI Team
